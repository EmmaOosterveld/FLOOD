{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLFOHVvq-kW8"
   },
   "source": [
    "# Project DSAIE: FLOOD1 CNN-Model\n",
    "The main task of the neural network is to predict water depth in meters based on an unseen Digital Elevation Map (DEM).\n",
    "\n",
    "The U-Net network is selected to model the water depth as it can work well with gridded data. The U-Net is based on a fully convolutional neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvYF5Vgm-kXG"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KxiBxwYW-kXI"
   },
   "outputs": [],
   "source": [
    "## Useful libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import time\n",
    "\n",
    "from cycler import cycler\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the color scheme\n",
    "sns.set_theme()\n",
    "colors = ['#0076C2', '#EC6842', '#A50034', '#009B77', '#FFB81C', '#E03C31', '#6CC24A', '#EF60A3', '#0C2340', '#00B8C8', '#6F1D77']\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lydia\\OneDrive - Delft University of Technology\\MasterEE\\CEGM3002 Data Science and Artifical Intelligence for Engineers\\Project\n",
      "[WinError 2] Het systeem kan het opgegeven bestand niet vinden: 'model'\n",
      "C:\\Users\\lydia\\OneDrive - Delft University of Technology\\MasterEE\\CEGM3002 Data Science and Artifical Intelligence for Engineers\\Project\n",
      "C:\\Users\\lydia\\OneDrive - Delft University of Technology\\MasterEE\\CEGM3002 Data Science and Artifical Intelligence for Engineers\n",
      "[WinError 3] Het systeem kan het opgegeven pad niet vinden: 'Data/Preprocessed_data'\n",
      "C:\\Users\\lydia\\OneDrive - Delft University of Technology\\MasterEE\\CEGM3002 Data Science and Artifical Intelligence for Engineers\n"
     ]
    }
   ],
   "source": [
    "#Change this cell to have the local path to the correct folders\n",
    "\n",
    "# go to FLOOD/CNN_model\n",
    "%cd ..\n",
    "# go to FLOOD/_model/model\n",
    "folder_path = 'model'\n",
    "%cd \"$folder_path\"\n",
    "import CNN_UNet as unet\n",
    "\n",
    "# go to FLOOD/CNN_model\n",
    "%cd ..\n",
    "# go to FLOOD/CNN_model/Data/Preprocessed_data\n",
    "folder_path = 'Data/Preprocessed_data'\n",
    "%cd \"$folder_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVPkbHIZ-kXM",
    "tags": []
   },
   "source": [
    "## Training dataset and selection of test dataset\n",
    "\n",
    "As a first step the datasets are loaded. The data is pre-processed before, so it can be downloaded directly.\n",
    "In the cell below one test dataset should be selected for testing the performance of the model.\n",
    "- test dataset 1: 20 DEM and WD with grid size of 64x64 km, time scale 4 days and starting point flood not changing\n",
    "- test dataset 2: 20 DEM and WD with grid size of 64x64 km, time scale 4 days and starting point flood changing\n",
    "- test dataset 3: 20 DEM and WD with grid size of 128x128 km, time scale 10 days and starting point flood changing\n",
    "\n",
    "The training data consists of 80 DEM locations and corresponding waterdepth (WD).\n",
    "\n",
    "It is important to mention that in order to be able to test dataset 3, the dataset is cut off to the first 4 days (instead of 10 days).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nCIVBmVy-kXO"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_dataset_CNN_time_two_inputs.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_dataset_CNN_time_two_inputs.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#select the test dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_dataset_CNN_time_two_inputs.pt'"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.load('train_dataset_CNN_time_two_inputs.pt')\n",
    "\n",
    "#select the test dataset\n",
    "dataset = 1\n",
    "\n",
    "test_dataset = torch.load('test_dataset_1_CNN_time_two_inputs.pt')\n",
    "#test_dataset = torch.load('test_dataset_2_CNN_time_two_inputs.pt')\n",
    "#test_dataset = torch.load('test_dataset_3_CNN_time_two_inputs.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8CzMpMR-kXQ"
   },
   "source": [
    "### Plot the inputs and outputs for one example.\n",
    "\n",
    "The inputs and outputs are as follows:\n",
    "\n",
    "Inputs:\n",
    "- digital elevation model (DEM)\n",
    "- Water depth (WD) for the first time step\n",
    "\n",
    "Output:\n",
    "- Water depth for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "xgQ9fQaD-kXR",
    "outputId": "26776bc8-fb96-4675-cad3-fbf17ece0fef"
   },
   "outputs": [],
   "source": [
    "inputs, outputs = train_dataset[0]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 7))\n",
    "\n",
    "axs[0].imshow(inputs[0].cpu(), cmap='terrain', origin='lower')\n",
    "axs[0].set_title('DEM')\n",
    "\n",
    "axs[1].imshow(inputs[1].cpu(), cmap='Blues', origin='lower')\n",
    "axs[1].set_title('Water depth [m] first time step')\n",
    "\n",
    "axs[2].imshow(outputs[-1].cpu(), cmap='Blues', origin='lower')\n",
    "axs[2].set_title('Water depth [m]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOcjDktm-kXY"
   },
   "source": [
    "### Normalization\n",
    "\n",
    "Since the input and output values may have very different ranges, it is important to perform normalization to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0r1jL7bt-kXZ"
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset, scaler_x, scaler_y):\n",
    "    min_x, max_x = scaler_x.data_min_[0], scaler_x.data_max_[0]\n",
    "    min_y, max_y = scaler_y.data_min_[0], scaler_y.data_max_[0]\n",
    "    normalized_dataset = []\n",
    "    for idx in range(len(dataset)):\n",
    "        x = dataset[idx][0]\n",
    "        y = dataset[idx][1]\n",
    "        norm_x = (x - min_x) / (max_x - min_x)\n",
    "        norm_y = (y - min_y) / (max_y - min_y)\n",
    "        normalized_dataset.append((norm_x, norm_y))\n",
    "    return normalized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3I3R1pP-kXc"
   },
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "for idx in range(len(train_dataset)):\n",
    "    scaler_x.partial_fit(train_dataset[idx][0].reshape(inputs.shape[0], -1).T.cpu())\n",
    "    scaler_y.partial_fit(train_dataset[idx][1].reshape(-1, 1).cpu())\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_dataset, scaler_x, scaler_y)\n",
    "normalized_test_dataset = normalize_dataset(test_dataset, scaler_x, scaler_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN4OfaBt-tLh"
   },
   "source": [
    "The training data is splitted in a training and validation part. The model is trained on the training data (80%) and overfitting is prevented by using validation data (20%). Every time the code is runned, a different split between validation and training data is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wNYVtQ2-kXe"
   },
   "outputs": [],
   "source": [
    "# Split dataset into train, validation\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(normalized_train_dataset, [train_size, val_size]) #different split for each run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ilFDdC1-kXg"
   },
   "source": [
    "# Model\n",
    "\n",
    "The U-Net network is selected to model the water depth as it can work well with gridded data. The U-Net is based on a fully convolutional neural network. To speed up the running time, the model should be runned on a GPU.\n",
    "\n",
    "The network has an encoder and decoder part. In this encoder part the spatial information is reduced while the feature information is increased. The double convolution consists of twice a 2D convolution followed by batch normalization, a ReLU activation function and a max pooling operation for downsampling the spatial dimension. The batch normalization normalizes the input of each layer within a mini-batch, to address for variance explosion.\n",
    "A choice was made to use ReLU6 as an activation function. ReLU6 limits the output to a maximum size of 6. This means that any input below 0 will be removed, while any input above 6 is converted to 6. ReLU6 improves the robustness of low-precision computation. The model performed better when using ReLU6 compared to ReLU.\n",
    "After this step the data downsampled using 5 max pooling layers. In this encoder the spatial information is reduced while the feature information is increased.\n",
    "\n",
    "The decoder consists of a sequence of up-convolutions. The data upsampled 5 times and goes trough the double convlution function every time to reduce the number of channels.\n",
    "\n",
    "Input in the U-Net network:\n",
    "- Series with DEMs\n",
    "- corresponding first time waterdpeth (WD)\n",
    "\n",
    "Targets:\n",
    "- Waterdepths at 96 timesteps(97 - first time step), correspinding to 4 days if 30-minute measurements.\n",
    "\n",
    "The model is based on one step forecast, where all targets are predicted in one go. There are two input channels in the network as there are two inputs and 96 classes as there are 96 timesteps to predict per DEM (targets).\n",
    "The  number of input and output channels per layer is defined by calibrating the network to find the optimal values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmqJ-UF2-kXn"
   },
   "outputs": [],
   "source": [
    "model = unet.UNet(n_channels = 2, n_classes = 96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DutAFmlA-kXp"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KXIJBcRJglr"
   },
   "source": [
    "The model is trained with the 'train_epoch' function below. The losses are computed with the mean squared error (MSE) with the prediction data and targets. The loss is minimized by optimizing the weights in the network. The evaluation function is used to compute the losses for the validation data. The losses are computed the same way as in training. However, the weights are not optimized in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_KfeSzK-kXp"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train() # specifies that the model is in training mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "\n",
    "        # Model prediction\n",
    "        preds = model(x)\n",
    "\n",
    "        # MSE loss function\n",
    "        loss = nn.MSELoss()(preds, y)\n",
    "\n",
    "        losses.append(loss.cpu().detach())\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        loss.backward()   # compute the gradients using backpropagation\n",
    "        optimizer.step()  # update the weights with the optimizer\n",
    "        optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJUNthfk-kXq"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, loader, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            y = batch[1]\n",
    "\n",
    "            # Model prediction\n",
    "            preds = model(x)\n",
    "\n",
    "            # MSE loss function\n",
    "            loss = nn.MSELoss()(preds, y)\n",
    "            losses.append(loss.cpu().detach())\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJz_O08Z-kXs"
   },
   "source": [
    "### Define the training paramters, the optimizer, and the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Va10ozzKslB"
   },
   "source": [
    "The three hyperparameters (learning rate, batch size and number of epochs) are calibrated by hand. The values below resulted in the best performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLX-pLAj-kXt"
   },
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.001 #proven to be best value\n",
    "batch_size = 8\n",
    "num_epochs = 300\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(normalized_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UbDz_Jc-kXv"
   },
   "source": [
    "## Training and validating\n",
    "\n",
    "The next step is to train the U-Net model for a number of epochs.\n",
    "During training the model with the training and validation data, both the training and test losses are recorded. To compare the models based on runtimes the 'start_time' and 'end_time' variables are added, which calculates the run time of the training part.\n",
    "For each epoch the 'train_epoch' function is used, which returns the train_loss. The evaluation function calculates the validation loss for each epoch.\n",
    "If for an epoch the validation loss is lower than the 'best_loss' so far, the 'best_loss' is updated. In the 'best_epoch' the best epoch number is saved.\n",
    "If the best_loss does not change for more then 25 epoch the model is stopped (early stopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3dirPE3-kXw",
    "outputId": "c02562e3-7028-4e03-fc12-d634d1cd563c"
   },
   "outputs": [],
   "source": [
    "# Lists to store losses and accuracies\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Keep track of the best validation accuracy and save the best model\n",
    "best_loss = np.inf #set to infinity to make sure the loss of the first epoch is saved\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# Parameter to calculate the runtime of the model\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    training_losses.append(train_loss)\n",
    "    validation_loss = evaluation(model, val_loader)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "    if best_loss > validation_loss:\n",
    "        print(f'Lowest loss updated at epoch {epoch}: {validation_loss:.5f}')\n",
    "        best_loss = validation_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    if epoch - best_epoch > 25: #stop if the validation loss has not improved for more than 25 epochs\n",
    "        break\n",
    "\n",
    "  # print all losses every fifth epoch\n",
    "    if epoch%5 == 0:\n",
    "        print(\"epoch:\",epoch, \"\\t training loss:\", np.round(train_loss,5), # we print loss and not accuracy\n",
    "                              \"\\t validation loss:\", np.round(validation_loss,5),\n",
    "                              \"\\t best loss:\", np.round(best_loss, 5))\n",
    "\n",
    "# Check the run time of the model\n",
    "end_time = time.time()\n",
    "run_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVFnbaixwXmt",
    "outputId": "01d2be81-84f0-4af3-afbd-3faedec11132"
   },
   "outputs": [],
   "source": [
    "# Load the best model after training is complete\n",
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ivy5Kjo0-kXy",
    "outputId": "4abdf448-74b2-4683-f4fa-19a89733f273"
   },
   "outputs": [],
   "source": [
    "# print the test loss for the selected test dataset\n",
    "model.to('cpu')\n",
    "test_loss = evaluation(model, test_loader)\n",
    "print(f'mean test_loss: {test_loss}')\n",
    "\n",
    "#general performance of the model\n",
    "print(f'runtime in seconds: {run_time:.2f}')\n",
    "print(f'validation loss last epoch: {validation_losses[-1]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8tajpAy-kXz"
   },
   "source": [
    "# Visualize results\n",
    "\n",
    "In this part the performance of the trained model are visualized. First the training and validation losses are shown in a figure. From this it can become clear if both losses are descreasing over the epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jl2-W3--kX0"
   },
   "source": [
    "## Losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "i-9pONHz-kX1",
    "outputId": "18e60bcb-cb46-436b-c568-b1e92ac00059"
   },
   "outputs": [],
   "source": [
    "plt.plot(training_losses, label='Training')\n",
    "plt.plot(validation_losses, label='Validation')\n",
    "plt.yscale('log')\n",
    "plt.title('CNN Losses without data augmentation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy3e723V-kX2"
   },
   "source": [
    "## Visualize the test dataset predictions\n",
    "\n",
    "To have a visual idea of the quality of the predictions for the test dataset a random id of the test dataset is selected and visualized for three timesteps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "id": "kAGeZe6C-kX8",
    "outputId": "f6698b1b-8e98-4ebd-8b89-11f5ec25f810"
   },
   "outputs": [],
   "source": [
    "time_step = [1, 37, 95] #Just for visualization three time_steps are selected to have an idea of the developed over time\n",
    "\n",
    "for i in time_step:\n",
    "    data_id = 4 #random number of one of the test dataset id\n",
    "    x = normalized_test_dataset[data_id][0].unsqueeze(0)\n",
    "    WD = normalized_test_dataset[data_id][1]\n",
    "    pred_WD = model(x).detach()\n",
    "\n",
    "    if dataset == 3: #different reshape values\n",
    "        DEM = scaler_x.inverse_transform(x[0].reshape(2,-1).T.cpu())[:,0].reshape(128,128)\n",
    "        real_WD = scaler_y.inverse_transform(WD[i].reshape(-1,1).cpu()).reshape(128,128)\n",
    "        pred_WD = scaler_y.inverse_transform(pred_WD[0][i].reshape(-1,1).cpu()).reshape(128,128)\n",
    "    else:\n",
    "        DEM = scaler_x.inverse_transform(x[0].reshape(2,-1).T.cpu())[:,0].reshape(64,64)\n",
    "        real_WD = scaler_y.inverse_transform(WD[i].reshape(-1,1).cpu()).reshape(64,64)\n",
    "        pred_WD = scaler_y.inverse_transform(pred_WD[0][i].reshape(-1,1).cpu()).reshape(64,64)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "    max_WD = max(pred_WD.max(), real_WD.max())\n",
    "    axs[0].imshow(DEM.squeeze(), cmap='terrain', origin='lower')\n",
    "    axs[1].imshow(real_WD.squeeze(), vmin = 0, vmax=max_WD, cmap='Blues', origin='lower')\n",
    "    axs[2].imshow(pred_WD.squeeze(), vmin = 0, vmax=max_WD,cmap='Blues', origin='lower')\n",
    "\n",
    "    plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = DEM.min(), vmax=DEM.max()),\n",
    "                                    cmap='terrain'), fraction=0.05, shrink=0.8, ax=axs[0])\n",
    "    plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_WD),\n",
    "                                    cmap='Blues'), fraction=0.05, shrink=0.8, ax=axs[1])\n",
    "    plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_WD),\n",
    "                                    cmap='Blues'), fraction=0.05, shrink=0.8, ax=axs[2])\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "    axs[0].set_title('Input DEM [m]')\n",
    "    axs[1].set_title(f'Target Real WD [m] after {(i / 2) + 0.5} h')\n",
    "    axs[2].set_title(f'Predicted WD [m] after {(i / 2) + 0.5} h')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulPXYhgK-kX9"
   },
   "source": [
    "To include all datasets in the testset the figure below shows the loss for each testing set. From this the range of test losses is visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e8f_EvZ-kX-"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "\n",
    "        # Model prediction\n",
    "        preds = model(x)\n",
    "        all_preds.append(preds)\n",
    "\n",
    "# concatenate all predictions\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "\n",
    "# select all outputs from test dataset\n",
    "test_WD = torch.stack([normalized_test_dataset[i][1] for i in range(len(normalized_test_dataset))])\n",
    "\n",
    "# loss on the test dataset per sample\n",
    "test_loss = torch.stack([nn.MSELoss()(all_preds[i], test_WD[i]) for i in range(len(all_preds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "eamGwPe3-kX_",
    "outputId": "70093467-d289-40e1-9f11-c17341e10187"
   },
   "outputs": [],
   "source": [
    "plt.plot(test_loss.cpu())\n",
    "plt.title('Test loss per sample')\n",
    "plt.xlabel('Sample id');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTA4SnxaMfun",
    "outputId": "394718b0-973c-4969-e5d2-362cc6620afd"
   },
   "outputs": [],
   "source": [
    "#Values of the test loss from the graph above\n",
    "print(f'Minimum test loss: {test_loss.min():.5f} and maximum test loss: {test_loss.max():.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX3VV4Y--kYC"
   },
   "source": [
    "### Visualize training data\n",
    "\n",
    "As an extra check of the quality of the model two training DEM's are shown with their corresponding target en predicted WD map. Based on this it can be visualy inspected if the model training performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "id": "JPWQlnPl-kYE",
    "outputId": "b1d7d1f3-63e1-4591-f83b-3316599d2b0e"
   },
   "outputs": [],
   "source": [
    "time_step = 95 #Visualizing only the last time step\n",
    "\n",
    "data_id = [1, 4] #two random sample id are selected\n",
    "\n",
    "for i in data_id:\n",
    "    x = normalized_train_dataset[i][0].unsqueeze(0)\n",
    "    WD = normalized_train_dataset[i][1]\n",
    "    pred_WD = model(x).detach()\n",
    "\n",
    "    DEM = scaler_x.inverse_transform(x[0].reshape(2,-1).T.cpu())[:,0].reshape(64,64)\n",
    "    real_WD = scaler_y.inverse_transform(WD[time_step].reshape(-1,1).cpu()).reshape(64,64)\n",
    "    pred_WD = scaler_y.inverse_transform(pred_WD[0][time_step].reshape(-1,1).cpu()).reshape(64,64)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "    max_WD = max(pred_WD.max(), real_WD.max())\n",
    "\n",
    "    axs[0].imshow(DEM.squeeze(), cmap='terrain', origin='lower')\n",
    "    axs[1].imshow(real_WD.squeeze(), vmin = 0, vmax=max_WD, cmap='Blues', origin='lower')\n",
    "    axs[2].imshow(pred_WD.squeeze(), vmin = 0, vmax=max_WD,cmap='Blues', origin='lower')\n",
    "\n",
    "    plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = DEM.min(), vmax=DEM.max()),\n",
    "                              cmap='terrain'), fraction=0.05, shrink=0.9, ax=axs[0])\n",
    "    plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_WD),\n",
    "                              cmap='Blues'), fraction=0.05, shrink=0.9, ax=axs[1])\n",
    "    plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_WD),\n",
    "                              cmap='Blues'), fraction=0.05, shrink=0.9, ax=axs[2])\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "\n",
    "    print(f'MAP show training data for data_id {i}')\n",
    "    axs[0].set_title('DEM')\n",
    "    axs[1].set_title(f'Real WD (m) after {(time_step / 2) + 0.5} h')\n",
    "    axs[2].set_title(f'Predicted WD (m) after {(time_step / 2) + 0.5} h')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPX5Jf6fIKb0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "IvYF5Vgm-kXG",
    "1ilFDdC1-kXg"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb1b16fbf94c3a099de98626de7352088f13e98c2a0ec94a62819c39fd5389e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
