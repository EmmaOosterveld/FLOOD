{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "uM4u3twCFaHK",
   "metadata": {
    "id": "uM4u3twCFaHK"
   },
   "source": [
    "# GNN Optimizer\n",
    "This notebook runs the optimizer for the GNN model. With the use of Weights & Biases the optimal parameters are determined. Weights & Biases runs sweeps with different parameters while tracking the validation loss. With the data obtained from the runs, it can be determined which parameters play the most important roles and what their optimum values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914bbc7-54b6-4a9d-b747-e54383dc7fa7",
   "metadata": {
    "id": "2914bbc7-54b6-4a9d-b747-e54383dc7fa7"
   },
   "outputs": [],
   "source": [
    "pip install torch_geometric==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e7ac0-14ea-4697-820d-95d71871c99b",
   "metadata": {
    "id": "963e7ac0-14ea-4697-820d-95d71871c99b"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.nn import GCNConv, TAGConv, ChebConv\n",
    "import networkx as nx\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import timeit\n",
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f3bf6-767a-4a4a-a11b-f6b08d352ff2",
   "metadata": {
    "id": "a59f3bf6-767a-4a4a-a11b-f6b08d352ff2"
   },
   "source": [
    "## Data pre-processing\n",
    "Here the train and validation data is loaded and normalized. Since the effect of the parameters on the validation loss plays the main roll, the test datasets are not used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O65uDp0HJcyJ",
   "metadata": {
    "id": "O65uDp0HJcyJ"
   },
   "outputs": [],
   "source": [
    "folder_path = 'FLOOD/GNN_model/model'\n",
    "%cd \"$folder_path\"\n",
    "import GNN_model as gnn\n",
    "\n",
    "folder_path = 'FLOOD/GNN_model/processing'\n",
    "%cd \"$folder_path\"\n",
    "import GNN_preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44561551-8669-48f0-9e3d-526d9a036618",
   "metadata": {
    "id": "44561551-8669-48f0-9e3d-526d9a036618"
   },
   "outputs": [],
   "source": [
    "G = nx.grid_2d_graph(64, 64, create_using=nx.DiGraph)\n",
    "pos = {i:(x+0.5,y+0.5) for i, (x,y) in enumerate(G.nodes())} # Position mapping for grid layout\n",
    "mapping = dict(zip(G, range(0, G.number_of_nodes())))\n",
    "G = nx.relabel_nodes(G, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bb510-9338-448f-b9e8-3c3890d88ef9",
   "metadata": {
    "id": "d68bb510-9338-448f-b9e8-3c3890d88ef9"
   },
   "outputs": [],
   "source": [
    "folder_path = 'FLOOD/raw_dataset'\n",
    "%cd \"$folder_path\"\n",
    "\n",
    "# importing the training set (80 simulations) and adding them to graphs\n",
    "DEMS, WDS = pre.load_data(1, 80, folder_path, augmentation=True, augmentation_per=1)\n",
    "\n",
    "# Choose the same random state, so DEM and WD are splitted the same way, 30% for validation\n",
    "train_dataset_DEM, val_dataset_DEM = train_test_split(DEMS, test_size=0.3, random_state=42)\n",
    "train_dataset_WD, val_dataset_WD = train_test_split(WDS, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395928a6-bfda-4b47-97d4-1b0d42c18062",
   "metadata": {
    "id": "395928a6-bfda-4b47-97d4-1b0d42c18062"
   },
   "outputs": [],
   "source": [
    "# normalizing the DEMs\n",
    "DEM_scaler = pre.CustomMinMaxScaler_interpolation((0,1)) # custom minmax scaler\n",
    "DEM_scaler.fit(train_dataset_DEM)\n",
    "\n",
    "scaled_DEM_train = DEM_scaler.transform(np.array(train_dataset_DEM))\n",
    "scaled_DEM_val = DEM_scaler.transform(np.array(val_dataset_DEM))\n",
    "\n",
    "# normalizing the WDs\n",
    "WD_scaler = pre.CustomMinMaxScaler_interpolation((0,1)) # custom minmax scaler\n",
    "\n",
    "WD_scaler, train_dataset = pre.normalize_WD(WD_scaler, train_dataset_WD, scaled_DEM_train, G, pos, train=True)\n",
    "val_dataset = pre.normalize_WD(WD_scaler, val_dataset_WD, scaled_DEM_val, G, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8b41a-549d-4f0d-b2f0-3e60c674935b",
   "metadata": {
    "id": "aca8b41a-549d-4f0d-b2f0-3e60c674935b"
   },
   "source": [
    "## Log in into Weights & Biases\n",
    "Note that to run this notebook, you need a Weights & Biases account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad34e7a-9d76-464c-9662-969665a05217",
   "metadata": {
    "id": "5ad34e7a-9d76-464c-9662-969665a05217"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a428a6-8ebc-4c3b-8cab-a720bdee1a8b",
   "metadata": {
    "id": "50a428a6-8ebc-4c3b-8cab-a720bdee1a8b"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb6833-32b7-42a5-a5c3-6aae119c445a",
   "metadata": {
    "id": "bccb6833-32b7-42a5-a5c3-6aae119c445a"
   },
   "source": [
    "## Define sweep\n",
    "The sweep method chosen is 'random', to make sure many different combinations are tried. This then clearly shows which parameters are more important and what their optimal values are.\n",
    "\n",
    "Note a combination of high K, high batch size and high number of MLP layers leads to a crash (ran out of CUDA memory). The follow-up runs will then also crash, even the combination of parameters is less computationally expensive. Therefore it is recommended to test the highest computational expensive run first in the model notebook, before running the optimization notebook. It is does not crash in the model notebook, this notebook will also safely run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337713b-4c29-4634-91ac-48ab825e9904",
   "metadata": {
    "id": "3337713b-4c29-4634-91ac-48ab825e9904"
   },
   "outputs": [],
   "source": [
    "# Choose method for configurations\n",
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "# Choose metric for optimization\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "# Choose parameters to vary\n",
    "parameters_dict = {\n",
    "    'n_layers_MLP': {\n",
    "        'values': [2, 3, 4]\n",
    "        },\n",
    "    'n_layers_GNN': {\n",
    "        'values': [3, 4, 6, 8]\n",
    "        },\n",
    "    'K_tot': {\n",
    "        'values': [96, 120, 144, 168, 192]\n",
    "        },\n",
    "   'hidden_features': {\n",
    "        'values': [64, 128]\n",
    "        },\n",
    "    'convolution_type': {\n",
    "        'values': ['ChebConv', 'TAGConv', 'GCNConv']\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'values': [8, 16]\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "# Define parameters which are not altered during the sweep, but still want to be mentioned\n",
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 200}\n",
    "    })\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bee783-0753-4ec1-b921-5d7d9064444e",
   "metadata": {
    "id": "b8bee783-0753-4ec1-b921-5d7d9064444e"
   },
   "outputs": [],
   "source": [
    "# Define sweep id variable\n",
    "sweep_id = wandb.sweep(sweep_config, project='encoder_decoder_GNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a5c05-4a22-4cb0-bbac-1b9514373396",
   "metadata": {
    "id": "4e8a5c05-4a22-4cb0-bbac-1b9514373396"
   },
   "source": [
    "## Run the Sweep agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6a941-8fde-49ea-9750-e5e9dea90ad2",
   "metadata": {
    "id": "cab6a941-8fde-49ea-9750-e5e9dea90ad2"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_function, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        preds = model(batch)\n",
    "        loss = loss_function(preds, batch.y)\n",
    "\n",
    "        losses.append(loss.cpu().detach())\n",
    "\n",
    "        loss.backward()   # compute the gradients using backpropagation\n",
    "        optimizer.step()  # update the weights with the optimizer\n",
    "        optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
    "\n",
    "    return np.array(losses).mean()\n",
    "\n",
    "def evaluation(model, loader, loss_function, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "    losses = []\n",
    "\n",
    "    # Remove gradients computations since we are only evaluating and not training\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "\n",
    "            loss = loss_function(preds, batch.y)\n",
    "            losses.append(loss.cpu())\n",
    "\n",
    "    return np.array(losses).mean()\n",
    "\n",
    "# Set training parameters\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# This line is used to select GPU to train, if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "    # If called by wandb.agent, as below,\n",
    "    # this config will be set by Sweep Controller\n",
    "    config = wandb.config\n",
    "    K = config.K_tot // config.n_layers_GNN\n",
    "    network = gnn.build_network(config.hidden_features, config.n_layers_MLP, config.n_layers_GNN, K, config.convolution_type).to(device)\n",
    "    optimizer = torch.optim.AdamW(network.parameters(), lr=learning_rate)\n",
    "    batch_size = config.batch_size\n",
    "\n",
    "    # Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    best_val = np.inf # start with an infinite high validation loss, so it will be updated with the first computed loss\n",
    "    best_epoch = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train_epoch(network, train_loader, optimizer, loss_function, device=device)\n",
    "        val_loss = evaluation(network, val_loader, loss_function, device=device)\n",
    "        if val_loss < best_val: # update the best validation loss\n",
    "        best_val = val_loss\n",
    "        best_epoch = epoch\n",
    "        if epoch - best_epoch > 20: # stop if the validation loss was not improved for more than 20 epochs\n",
    "        break\n",
    "\n",
    "    wandb.log({'val_loss': best_val, 'best val at epoch': best_epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3770892-450e-4668-8c35-97b79b3702c9",
   "metadata": {
    "id": "b3770892-450e-4668-8c35-97b79b3702c9"
   },
   "outputs": [],
   "source": [
    "# Start the sweep, count tells how many configurations are going to be done\n",
    "wandb.agent(sweep_id, function=train, count=100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
